 ---
title: "SENTIMENT ANALYSIS DEATH NOTE"
author: "Rachael Ndindi"
date: "10/11/2021"
output:
  html_document:
    df_print: paged
---
```{r}
# Load libraries
#install.packages("RWeka") 
#install.packages("memery")
#install.packages("ggimage") 
#install.packages("magick")
#install.packages("tidyverse")
#install.packages("tm")
#install.packages("wordcloud")
#install.packages("wordcloud2")
#install.packages("tidytext")
#install.packages("reshape2")
#install.packages("knitr")
#install.packages("gridExtra")
#install.packages("magick")
#install.packages("memery")
#install.packages("ggimage")
#install.packages("igraph")
#install.packages("ggraph")
#install.packages("syuzhet")
#install.packages("textdata")
#install.packages("stringr")
#install.packages("readxl")
#install.packages("tidyr")
library(qdap)
library(tidyr)
library(data.table)
library(rJava)
library(readxl)
library(stringr)
library(tidyverse) 
library(tm) 
library(wordcloud)
library(tidytext) 
library(reshape2) 
library(RWeka) 
library(knitr) 
library(gridExtra) 
library(grid) 
library(magick) 
library(memery) 
library(ggimage) 
library(igraph) 
library(ggraph)
library(syuzhet)
library(textdata)
library(dplyr)
library(RColorBrewer)
library(usethis)
library(devtools)
library(wordcloud2)
library(extrafont)
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_301')
devtools::install_github("lchiffon/wordcloud2")

scripts=read_excel("C:/Users/Rachael/Desktop/Projects/Death Note.xlsx")
head(scripts)
bing = get_sentiments("bing")
nrc = get_sentiments("nrc") 
afinn = get_sentiments("afinn")

```
Some data cleaning
```{r}
scripts$Character.Name=str_trim(scripts$Character.Name)
scripts$Episode.Name=str_trim(scripts$Episode.Name)
scripts$Dialog=str_trim(scripts$Dialog)
scripts$Character.Name=str_to_title(scripts$Character.Name)
scripts$Episode.Name=str_to_title(scripts$Episode.Name)
scripts$Dialog =gsub("[\u2018\u2019\u201A\u201B\u2032\u2035]", "'", scripts$Dialog)
scripts$Character.Name = gsub("[\u2018\u2019\u201A\u201B\u2032\u2035]", "'", scripts$Character.Name)
count(scripts,Character.Name,sort = FALSE)
count(scripts,Episode.Name,sort = FALSE)
count(scripts,Episode.No,sort = FALSE)
head(scripts)
```

Text mining by removing punctuation, removing extra white space, making all characters lower case, remove numbers and remove common English words.

```{r}
v_stopwords = c(stopwords("english"), c("thats","weve","hes","theres","ive","im","will","can","cant","dont","youve","us",     "youre","youll","theyre","whats","didnt", "ill"))
scripts.corpus=VCorpus(VectorSource(scripts$Dialog))
scripts.corpus= scripts.corpus %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)  %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removeWords,v_stopwords)  %>%
  tm_map(removeWords,stopwords("SMART"))#%>%
  #tm_map(stemDocument, language="english")

tdm= TermDocumentMatrix(scripts.corpus) %>%
  as.matrix()
words=sort(rowSums(tdm),decreasing = TRUE)
df=data.frame(word=names(words), freq=words)
head(scripts)

```



Extract tokens for bigrams
```{r}
tokenizer2 = function(x) NGramTokenizer(x, Weka_control(min=2, max=2))

fbs.tdm = TermDocumentMatrix(scripts.corpus, control=list(tokenize=tokenizer2))
fbs.tdm = removeSparseTerms(fbs.tdm, 0.999)
fbs.tdm = as.matrix(fbs.tdm)
word_freqs = rowSums(fbs.tdm)
dm=as.list(word_freqs)
dm1 = data.frame(unlist(dm), stringsAsFactors = FALSE)
setDT(dm1,keep.rowname=TRUE)
setnames(dm1, 1, "term")
setnames(dm1, 2, "freq")
dm2=head(arrange(dm1, desc(freq)), n=20)

```


Trigram tokenizer for trigrams
```{r}
tokenizer3 = function(x) NGramTokenizer(x, Weka_control(min=3, max=3))

fts.tdm = TermDocumentMatrix(scripts.corpus, control=list(tokenize=tokenizer3))
fts.tdm = removeSparseTerms(fts.tdm, 0.999)
fts.tdm = as.matrix(fts.tdm)
word_freqs2 = rowSums(fts.tdm)
dm3=as.list(word_freqs2)
dm4 = data.frame(unlist(dm3), stringsAsFactors = FALSE)
setDT(dm4, keep.rownames = TRUE)
setnames(dm4,1,"term")
setnames(dm4, 2, "freq")
dm5=head(arrange(dm4, desc(freq)), n=20)

```



Top 19 characters with the most dialog
```{r}
scripts %>% 
  count(Character.Name) %>%
  arrange(desc(n)) %>% 
  slice(1:19) %>%
  mutate(Percentage=n/nrow(scripts)) %>%
  ggplot(aes(x=reorder(Character.Name, Percentage), y=Percentage)) +
  geom_bar(stat="identity", aes(fill=Percentage), show.legend=F) +
  geom_label(aes(label=paste0(round(Percentage*100, 2), "%"))) +
  scale_y_continuous(labels=scales::percent) +
  scale_fill_gradient(low="#58D68D", high="#239B56") +
  labs(x="Character", y="Dialogue Percentage", title="Most talkative characters in Death Note") +
  coord_flip() +
  theme_bw()

image = image_read("C:/Users/Rachael/Desktop/Projects/Ryuk.jpg")
grid.raster(image, x=0.865, y=0.305, height=0.4)
```

OPINION MINING
```{r}
tokens = scripts %>%  
  mutate(dialogue = as.character(scripts$Dialog)) %>%
  unnest_tokens(word, dialogue)
```


Use bing lexicon to compare words falling into the negative and positive categories
```{r}
tokens %>% 
  inner_join(bing, "word") %>%
  count(word, sentiment, sort=TRUE) %>% 
  acast(word ~ sentiment, value.var = "n", fill=0) %>% 
  comparison.cloud(colors=c("#991D1D", "#327CDE"), max.words = 100)

```

NRC Lexicons
```{r}
head(nrc)
sentiments = tokens %>% 
  inner_join(nrc, "word") %>%
  count(sentiment, sort=TRUE) 

ggplot(data=sentiments, aes(x=reorder(sentiment, n), y=n)) + 
  geom_bar(stat="identity", aes(fill=sentiment), show.legend=FALSE) +
  geom_label(label=sentiments$n) +
  labs(x="Sentiment", y="Frequency", 
       title="Death Note - Sentiment Analysis (NRC lexicon)") +
  coord_flip() +
  theme_bw()

image = image_read("C:/Users/Rachael/Desktop/Projects/Misa_and_Rem.jpg") 
grid.raster(image, x=0.905, y=0.28, height=0.34)
```


Top 10 frequent terms for each sentiment, NRC lexicon
```{r}
 tokens %>% 
  inner_join(nrc, "word") %>%
  count(sentiment, word, sort=T) %>%
  group_by(sentiment) %>%
  arrange(desc(n)) %>%
  slice(1:10) %>%
  ggplot(aes(x=reorder(word, n), y=n)) +
  geom_col(aes(fill=sentiment), show.legend=FALSE) +
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  facet_wrap(~sentiment, scales="free_y") +
  labs(y="Frequency", x="Words", 
       title="Most frequent terms for each sentiment (NRC lexicon)") +
  coord_flip() +
  theme_bw()

image = image_read("C:/Users/Rachael/Desktop/Projects/death note book.png") 
grid.raster(image, x=0.78, y=0.2, height=0.4)

```

Sentiment Analysis for the top 10 characters with the most dialogue, nrc lexicon
```{r}
tokens %>%
  filter(Character.Name %in% c("Light","L","Misa","Soichiro","Near", "Aizawa","Ryuk", "Higuchi","Rem","Takada")) %>%
  inner_join(nrc, "word") %>%
  count(Character.Name, sentiment, sort=TRUE) %>%
  ggplot(aes(x=sentiment, y=n)) +
  geom_col(aes(fill=sentiment), show.legend=FALSE) +
  facet_wrap(~Character.Name, scales="free_x") +
  labs(x="Sentiment", y="Frequency", 
       title="Sentiment Analysis for each character (NRC lexicon)") +
  coord_flip() +
  theme_bw()

image = image_read("C:/Users/Rachael/Desktop/Projects/death note book.png") 
grid.raster(image, x=0.78, y=0.2, height=0.4)


```

AFINN LEXICON
```{r}
plot = tokens %>% 
  inner_join(afinn, "word") %>% 
  count(value, sort=T) %>%
  ggplot(aes(x=value, y=n)) +
  geom_bar(stat="identity", aes(fill=n), show.legend=FALSE, width=0.5) +
  geom_label(aes(label=n)) +
  scale_fill_gradient(low="thistle1", high="thistle4") +
  scale_x_continuous(breaks=seq(-3, 4, 1)) +
  labs(x="Score", y="Frequency", title="Word distribution (AFINN lexicon)") +
  theme_bw() 


img = "C:/Users/Rachael/Desktop/Projects/L.png" 
lab = ""  
pos = list(w=1, h=0.75, x=0.5, y=0.5)
meme(img, lab, "L.png", inset=plot, inset_pos=pos)
meme = image_read("L.png")
plot(meme)

```

Word cloud
```{r}
image.colors=c("#952d40", "#d87eb1", "#7b7b7b","#6a1d31","#707f8a")
image.background="#000000"
wordcloud2(df,color = rep_len(image.colors, nrow(df)), backgroundColor = image.background, fontFamily="DM Sans", size=2.5, minSize=5, rotateRatio=0)
```


We are going to compute the most repeated bigrams and trigrams. A bigram is a sequence of two adjacent elements from a string of tokens, which are typically letters, syllables, or words. A trigram is the same but with three adjacent elements
```{r}


plot1 = ggplot(data=dm2, aes(x=reorder(term, freq), y=freq)) +  
  geom_bar(stat="identity", fill="#00FF00", colour="black") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(x="Bigram", y="Frequency") +
  coord_flip() + 
  ggtitle("Bigram for Death Note") 

plot2 = ggplot(data=dm5, aes(x=reorder(term, freq), y=freq)) +  
  geom_bar(stat="identity", fill="#FF9999", colour="black") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(x="Trigram", y="Frequency") +
  coord_flip() + 
  ggtitle("Trigram for Death Note") 


# Subplot
grid.arrange(plot1, plot2, ncol=2, top="Bigrams and Trigrams - Death Note")
```

Network of Bigrams
```{r}
bigrams_separated = dm2 %>%
                    separate(term, c("word1", "word2"), sep=" ")

bigrams_graph = bigrams_separated %>%
                filter(freq>3) %>%
                graph_from_data_frame() 
  
set.seed(2016)

a = grid::arrow(type="closed", length=unit(0.15, "inches"))

ggraph(bigrams_graph, layout="fr") +
  geom_edge_link(aes(edge_alpha=freq), show.legend = FALSE, arrow=a, end_cap=circle(0.07, 'inches')) +
  geom_node_point(color="lightblue", size=5) +
  geom_node_text(aes(label=name), vjust=1, hjust=1) +
  theme_void()
```








